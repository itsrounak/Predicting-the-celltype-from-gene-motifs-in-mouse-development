---
title: "ETC3250/5250 - Project 2022"
author: Rounak Agarwal
output:
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = FALSE,
  message = FALSE,
  warning = FALSE)
```

# Data pre-processing

### Explain what you did to the data to prepare for modeling, eg subsetting or transforming variables, filtering cases. 


> Firstly we merged the testing data's using the common variable "location".

````{r}
ts<- merge(x=ts,y=ts_sol, by="location", all.x = TRUE)
```

> Since XGBoost requires the classes to be in an integer format, starting with 0. So, we converted the first class to 0. The celltype factor is converted to the proper integer format.

```{r}
celltype<- tr$celltype

label_tr<- as.integer(tr$celltype)-1
tr$celltype<- NULL


label_ts<- as.integer(factor(ts$celltype))-1
ts$celltype<- NULL

train.data = as.matrix(tr[,-1])
train.label = label_tr
test.data = as.matrix(ts[,-1])
test.label = label_ts
```

> Next, we transformed the training and testing data sets into xgb.DMatrix objects that were later used for fitting the XGBoost model and predicting new outcomes. 

```{r}
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
```




# Summary of important variables

### Describe which variables did you find most useful for separating the classes. 

> I tried checking all the variables using the MeanDecreaseGini for their importance and found that all the variables were significant. (using the optimal number of mytry in the random forest function)

```{r}
mtry <- tuneRF(tr_new,tr_new$celltype, ntreeTry=500,
               stepFactor=1.5,improve=0.3, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]


mouse_rf <- randomForest(celltype~.,data=tr_new, mtry=best.m, importance=TRUE,ntree=800)

# Getting the MeanDecreaseGini values
imp <- importance(mouse_rf, type =2) %>% as.data.frame()
gini_score <- cbind(var = rownames(imp), imp)
```

> Therefore, we used all the variable provided to make our fit for prediction.

# Model performance

### What options on the model produced the best performance? What was your best model?

> I tried three different models (Random Forest , gradient boosting and XGBoost) and found XGBoost to be the most effective. 

> In the XGBoost model I trained the model using the "xgbTree" method fot the traing data and extracted the optimal values required for creating the fit. 

```{r}
# Train the model
model<- train( celltype~., data = tr[,-1], trControl= xgb_trcontrol, method= "xgbTree")

# Best values for hyperparameters
model$bestTune
```

> On extracting the bestTune I found that the tuning parameter 'gamma' was held constant at a value of 0. Tuning parameter 'min_child_weight' was held constant at a value of 1. Accuracy was used to select the optimal model using the largest value. The final values used for the model were nrounds = 150, max_depth = 1, eta =0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75. 

> I noted these values and put it in my fit to get the predictions.


```{r}

#The multi:softprob objective tells the algorithm to calculate probabilities for every possible outcome for every observation.

# Define the parameters for multinomial classification
num_class = length(levels(celltype))
params = list(
  booster="gbtree",
  eta=0.3,
  max_depth=1,
  gamma=0,
  subsample=0.75,
  colsample_bytree=0.6,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class
)
```

```{r}
# Train the XGBoost classifer
xgb.fit=xgb.train(
  params=params,
  data=xgb.train,
  nrounds=150,
  nthreads=1,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
)

```

## Reference: 

>Kube, D., 2019. XGBoost Multinomial Classification Iris Example in R. [online] Rstudio-pubs-static.s3.amazonaws.com. Available at: <https://rstudio-pubs-static.s3.amazonaws.com/456044_9c275b0718a64e6286751bb7c60ae42a.html> [Accessed 27 May 2022].

>Berhane, F., 2018. Extreme Gradient Boosting with R | DataScience+. [online] Datascienceplus.com. Available at: <https://datascienceplus.com/extreme-gradient-boosting-with-r/> [Accessed 27 May 2022].


